![logo](https://user-images.githubusercontent.com/72475872/149357794-9b05e5ac-c29b-431e-a4da-d3c9392d2d2b.png)


# About ProjectSOTS

ProjectSOTS is a research team that gathered by Assoc. Prof. Dr. Mehmet Fatih Amasyali under TUBITAK Project (Sort Optimization of Training Samples in Machine Learning, Project Number: 120E100 ).

Human learning is incremental (not all examples is given at once) and starts with simple examples. Our education systems are based on this hypothesis. It starts with basic concepts, then it goes into difficult ones. In the field of machine learning, this hypothesis was used to develop curriculum learning (CL) and its derivatives.

Our aims: Finding more generalizable Curriculum Learning methods, providing theoretical explanations for CL methods, examining the effect of using augmented data within these methods.


# Research Team
| *Advisor* | |
| :-: |  :-: |
| [Mehmet Fatih Amasyali](https://sites.google.com/view/mfatihamasyali/) | |
|  ***Researchers*** | |
| [Himmet Toprak Kesgin](https://avesis.yildiz.edu.tr/tkesgin)  |  [Işıl Berfin Koparan](https://github.com/isilberfin)  |
| [Besher Alkurdi](https://github.com/mrbesher/) |[Rameş Aliyev](https://rames.dev/) |
| [Tolga Recep Uçar]() |[Feyza Şahin]() |
| [Rayene Bech]() | [Somaiyeh Dehghan]() |
| [Ahmet Bağcı]() | [Kaan Sönmezöz]() |
| [Ahmet Topal]() | [Gülsüm Yiğit]() |
| [Melike Nur Mermer]() | [İpek Koç](https://github.com/ip-ek) |
| [Bahattin Cihan Ünal]() | [Buğra Hamza Gündoğ](https://github.com/BugraHamza)|
| [Şafak Bilici](https://github.com/safakkbilici) | [Ö. Faruk Cebeci]() |
| [Enes Dedeoğlu]()  | [Toygar Tanyel](https://github.com/Toygarr) |
| [Abdulsamet Aktaş](https://avesis.marmara.edu.tr/abdulsamet.aktas) | [Talha Bacak]() |


# Publications:

## Curriculum Learning:

Kesgin, H. T., & Amasyali, M. F. (2023). Cyclical curriculum learning. IEEE Transactions on Neural Networks and Learning Systems. [link](https://ieeexplore.ieee.org/abstract/document/10103632) [code](https://github.com/CyclicalCurriculum/Cyclical-Curriculum)

Dedeoglu, E., Kesgin, H. T., & Amasyali, M. F. (2024). A Robust Optimization Method for Label Noisy Datasets Based on Adaptive Threshold: Adaptive-k. Front. Comput. Sci., 2024, 18(4): 184315. [link](https://journal.hep.com.cn/fcs/EN/10.1007/s11704-023-2430-4)

Yigit, G., & Amasyali, M. F. (2023). Enhancing multiple-choice question answering through sequential fine-tuning and Curriculum Learning strategies. Knowledge and Information Systems, 1-18. [link](https://link.springer.com/article/10.1007/s10115-023-01918-2)

Dehghan, S., & Amasyali, M. F. (2023). SelfCCL: Curriculum Contrastive Learning by Transferring Self-Taught Knowledge for Fine-Tuning BERT. Applied Sciences, 13(3), 1913. [link](https://www.mdpi.com/2076-3417/13/3/1913)

Yeğin, M. N., Kurttekin, Ö., Bahşi, S. K., & Amasyali, M. F. Training with growing sets: A comparative study. Expert Systems, e12961. [link](https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12961)

Yigit, G., & Amasyali, M. F. (2021). Assessing the impact of minor modifications on the interior structure of GRU: GRU1 and GRU2. Concurrency and Computation: Practice and Experience, e6775. [link](https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6775)

Bech, R., Sahin, F., & Amasyali, M. F. (2022, September). Improving Abstractive Summarization for the Turkish Language. In 2022 Innovations in Intelligent Systems and Applications Conference (ASYU) (pp. 1-6). IEEE. [link](https://ieeexplore.ieee.org/abstract/document/9925328)

## Data Augmentation:

Alkurdi, B., Sarioglu, H. Y., & Amasyali, M. F. (2022, December). Semantic Similarity Based Filtering for Turkish Paraphrase Dataset Creation. In Proceedings of the 5th International Conference on Natural Language and Speech Processing (ICNLSP 2022) (pp. 119-127). [link](https://aclanthology.org/2022.icnlsp-1.14.pdf)

Bağcı, Ahmet, and Mehmet Fatih Amasyali. "Comparison of Turkish Paraphrase Generation Models." 2021 International Conference on INnovations in Intelligent SysTems and Applications (INISTA). IEEE, 2021. [link](https://ieeexplore.ieee.org/abstract/document/9548335)

Bilici, M. Şafak, and Mehmet Fatih Amasyali. "Variational Sentence Augmentation for Masked Language Modeling." 2021 Innovations in Intelligent Systems and Applications Conference (ASYU). IEEE, 2021. [link](https://ieeexplore.ieee.org/abstract/document/9599089) 

Sonmezoz, Kaan, and Mehmet Fatih Amasyali. "Same Sentence Prediction: A new Pre-training Task for BERT." 2021 Innovations in Intelligent Systems and Applications Conference (ASYU). IEEE, 2021. [link](https://ieeexplore.ieee.org/abstract/document/9598954)

Topal, Ahmet, and Mehmet Fatih Amasyali. "When does Synthetic Data Generation Work?." 2021 29th Signal Processing and Communications Applications Conference (SIU). IEEE, 2021. [link](https://ieeexplore.ieee.org/abstract/document/9477956)

## Other

Kesgin, H. T., & Amasyali, M. F. (2022, September). Investigating Semi-Supervised Learning Algorithms in Text Datasets. In 2022 Innovations in Intelligent Systems and Applications Conference (ASYU) (pp. 1-6). IEEE. [link](https://ieeexplore.ieee.org/abstract/document/9925410)

Poyraz, Y. Z., Tugcu, M., & Amasyali, M. F. (2022, September). Improving BERT Pre-training with Hard Negative Pairs. In 2022 Innovations in Intelligent Systems and Applications Conference (ASYU) (pp. 1-6). IEEE. [link](https://ieeexplore.ieee.org/abstract/document/9925395)

## REPOS

Curriculum Learning

https://github.com/CyclicalCurriculum/Cyclical-Curriculum<br />
https://github.com/projectSOTS/SOTS_toprak_1<br />
https://github.com/projectSOTS/SOTS_ipek_1

Text Data Augmentation

https://github.com/mrbesher/semantic-filtering-for-paraphrasing<br />
https://github.com/projectSOTS/SOTS_bugra_1<br />
https://github.com/Toygarr/synthetic-text-data-augmentation

### Data sets

[Kaggle](https://www.kaggle.com/toygarr/datasets-for-natural-language-processing)


